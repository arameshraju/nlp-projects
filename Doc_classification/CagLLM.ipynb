{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d993e6",
   "metadata": {},
   "source": [
    "# Document Classifcaiton using Open LLM models  \n",
    "# Context-Augmented Generation (CAG) \n",
    "CAG is a prompt engineering technique where a language model is provided with explicit context before being asked to perform a task. In the context of document classification, CAG works as follows:\n",
    "\n",
    "1. Context Construction:\n",
    "\n",
    "    - The notebook reads a JSON file (classification.json) containing document types and their descriptions.\n",
    "    - It constructs a prompt that lists all possible document types and their descriptions as context.\n",
    "2. Prompting the LLM:\n",
    "\n",
    "    - The document to be classified is appended to the prompt.\n",
    "    - The prompt asks the LLM to classify the document and return only the type.\n",
    "3. LLM Inference:\n",
    "\n",
    "    - The prompt is sent to an open-source LLM (via Ollama).\n",
    "    - The LLM uses the provided context to reason and select the most appropriate document type.\n",
    "\n",
    "Key Points\n",
    "    - No Retrieval Step:\n",
    "        Unlike RAG (Retrieval-Augmented Generation), CAG does not use vector search or embeddings. All possible classes are presented in the prompt.\n",
    "    - Simplicity:\n",
    "        CAG is easy to implement and works well when the number of classes is small.\n",
    "    - Limitations:\n",
    "        For a large number of classes or long descriptions, the prompt may become too long for the LLM to handle efficiently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb24cb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                ID              SIZE      MODIFIED          \n",
      "llama3:latest       365c0bd3c000    4.7 GB    52 minutes ago       \n",
      "gemma3:latest       a2af6cc3eb7f    3.3 GB    About an hour ago    \n",
      "phi4-mini:latest    78fad5d182a7    2.5 GB    2 hours ago          \n",
      "mistral:7b          f974a74358d6    4.1 GB    7 days ago           \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "903a0620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama \n",
    "import json \n",
    "import time \n",
    "import os\n",
    "\n",
    "\n",
    "def getModelResponse(document_text, model=\"phi4-mini\",classification_json_path='data/classification.json'):\n",
    "    \"\"\"\n",
    "    Gets a classification response from the Ollama model using context from classification.json.\n",
    "    Returns a dict with 'type' and 'score'.\n",
    "    \"\"\"\n",
    "    with open(classification_json_path, 'r') as f:\n",
    "        classification_context = json.load(f)\n",
    "    \n",
    "    context_str = \"Document Types and Descriptions:\\n\"\n",
    "    for doc_type, desc in classification_context.items():\n",
    "        context_str += f\"- {doc_type}: {desc}\\n\"\n",
    "    \n",
    "    prompt = (\n",
    "        f\"{context_str}\\n\"\n",
    "        \"Given the above document types, classify the following document and return only the type:\\n\\n\"\n",
    "        f\"{document_text}\"\n",
    "    )\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': prompt},\n",
    "        ]\n",
    "    )\n",
    "    doc_type = response['message']['content']\n",
    "\n",
    "    return  doc_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea4b804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi4-mini Document Type: Resume  Execution Duration: 14.95 seconds\n",
      "llama3 Document Type: Resume  Execution Duration: 34.15 seconds\n",
      "gemma3 Document Type: Resume  Execution Duration: 0.45 seconds\n",
      "mistral:7b Document Type:  Resume  Execution Duration: 0.48 seconds\n",
      "\t\t****** end o file process data/file1.txt *******\n",
      "\n",
      "phi4-mini Document Type: Job Post  Execution Duration: 15.03 seconds\n",
      "llama3 Document Type: Job Post  Execution Duration: 30.52 seconds\n",
      "gemma3 Document Type: Job Post  Execution Duration: 8.28 seconds\n",
      "mistral:7b Document Type:  Job Post  Execution Duration: 20.19 seconds\n",
      "\t\t****** end o file process data/file2.txt *******\n",
      "\n",
      "phi4-mini Document Type: Job Post  Execution Duration: 13.90 seconds\n",
      "llama3 Document Type: Letter  Execution Duration: 27.44 seconds\n",
      "gemma3 Document Type: Letter  Execution Duration: 6.78 seconds\n",
      "mistral:7b Document Type:  The provided document is a Job Post or Job Offer.  Execution Duration: 17.94 seconds\n",
      "\t\t****** end o file process data/file3.txt *******\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "files = ['file1.txt', 'file2.txt', 'file3.txt']\n",
    "models = ['phi4-mini', 'llama3', 'gemma3','mistral:7b']\n",
    "# models = ['llama3']\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    file_path = f'data/{file}'\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read().strip()\n",
    "\n",
    "        for model in models:\n",
    "            start_time = time.time()\n",
    "            doctype = getModelResponse( content, model)\n",
    "            end_time = time.time()\n",
    "            print(f\"{model} Document Type: {doctype}  Execution Duration: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "        print(f\"\\t\\t****** end o file process {file_path} *******\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
