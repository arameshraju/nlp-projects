{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d993e6",
   "metadata": {},
   "source": [
    "# PHI \n",
    "!ollama pull phi4-mini\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb24cb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                ID              SIZE      MODIFIED       \n",
      "llama3:latest       365c0bd3c000    4.7 GB    7 minutes ago     \n",
      "gemma3:latest       a2af6cc3eb7f    3.3 GB    15 minutes ago    \n",
      "phi4-mini:latest    78fad5d182a7    2.5 GB    49 minutes ago    \n",
      "mistral:7b          f974a74358d6    4.1 GB    7 days ago        \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "903a0620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama \n",
    "import json \n",
    "import time \n",
    "import os\n",
    "\n",
    "\n",
    "def getModelResponse(document_text, model=\"phi4-mini\",classification_json_path='data/classification.json'):\n",
    "    \"\"\"\n",
    "    Gets a classification response from the Ollama model using context from classification.json.\n",
    "    Returns a dict with 'type' and 'score'.\n",
    "    \"\"\"\n",
    "    with open(classification_json_path, 'r') as f:\n",
    "        classification_context = json.load(f)\n",
    "    \n",
    "    context_str = \"Document Types and Descriptions:\\n\"\n",
    "    for doc_type, desc in classification_context.items():\n",
    "        context_str += f\"- {doc_type}: {desc}\\n\"\n",
    "    \n",
    "    prompt = (\n",
    "        f\"{context_str}\\n\"\n",
    "        \"Given the above document types, classify the following document and return only the type:\\n\\n\"\n",
    "        f\"{document_text}\"\n",
    "    )\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': prompt},\n",
    "        ]\n",
    "    )\n",
    "    doc_type = response['message']['content']\n",
    "    score = 1.0  # Placeholder score, replace with actual logic if available\n",
    "\n",
    "    return {'type': doc_type, 'score': score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi4-mini Document Type: {'type': 'Resume', 'score': 1.0}  Execution Duration: 0.28 seconds\n",
      "             ******* \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: \"data/<_io.TextIOWrapper name='data/file1.txt' mode='r' encoding='cp1252'>\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m start_time = time.time()\n\u001b[32m     12\u001b[39m file_path = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     14\u001b[39m     content = file.read().strip()\n\u001b[32m     16\u001b[39m doctype = getModelResponse( content, model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\new_works\\nlp-projects\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:327\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    322\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    325\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 22] Invalid argument: \"data/<_io.TextIOWrapper name='data/file1.txt' mode='r' encoding='cp1252'>\""
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "import time\n",
    "\n",
    "files = ['file1.txt', 'file2.txt', 'file3.txt']\n",
    "models = ['phi4-mini', 'llama3', 'gemma3','mistral:7b']\n",
    "# models = ['llama3']\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    file_path = f'data/{file}'\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read().strip()\n",
    "\n",
    "        for model in models:\n",
    "            start_time = time.time()\n",
    "            doctype = getModelResponse( content, model)\n",
    "            end_time = time.time()\n",
    "            print(f\"{model} Document Type: {doctype}  Execution Duration: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "        print(\"\\t\\t\\t******* \")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
