{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fb6ad0f",
   "metadata": {},
   "source": [
    "# LLM\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) for document classification. The workflow leverages ChromaDB for vector storage and retrieval, Sentence Transformers for embedding, and Ollama for LLM inference.\n",
    "\n",
    "## 1. Embedding and Storing Document Types\n",
    "- Functionality: The code reads a JSON file (classification.json) containing document types and their descriptions. Each description is embedded using a Sentence Transformer model (all-MiniLM-L6-v2 by default).\n",
    "- Storage: The embeddings, along with metadata, are stored in a ChromaDB collection called document_types. This enables efficient similarity search later.\n",
    "\n",
    "## 2.  Retrieval-Augmented Classification\n",
    "- Embedding Input: When a new document needs to be classified, its text is embedded using the same Sentence Transformer model.\n",
    "- Retrieval: The embedding is used to query ChromaDB for the top-k most similar document type descriptions.\n",
    "- Prompt Construction: The retrieved descriptions are formatted into a context string, which is then included in a prompt for the LLM.\n",
    "- LLM Inference: The prompt is sent to an Ollama model (e.g., phi4-mini, llama3, gemma3, mistral:7b). The LLM is asked to classify the document based on the retrieved context and return only the document type.\n",
    "\n",
    "## Sample data :  \n",
    "- Classifcaiton \n",
    "```json\n",
    "{\n",
    "  \"Invoice\": \"Contains billing details, amounts, due dates, sender/receiver info, itemized lists.\",\n",
    "  \"Contract\": \"Includes legal terms, parties involved, signatures, obligations, clauses.\",\n",
    "  \"Resume\": \"Lists work experience, education, skills, personal contact information.\",\n",
    "  \"Email\": \"Contains sender, recipient, subject, body, and often a conversational tone.\",\n",
    "  \"Report\": \"Summarizes findings, includes data analysis, conclusions, and recommendations.\",\n",
    "  \"Letter\": \"Formal or informal communication, often includes a greeting, body, and closing.\",\n",
    "  \"Presentation\": \"Visual slides with text, images, and charts to convey information or ideas.\",\n",
    "  \"Proposal\": \"Outlines a plan or suggestion, often includes objectives, methods, and costs.\",\n",
    "  \"Job Post\": \"Describes a job opening, including role, responsibilities, qualifications, location, and company details.\"\n",
    "}\n",
    "```\n",
    "\n",
    "### inputfiles \n",
    "- file1.txt -   Resume \n",
    "- file2.txt -  Job Posting \n",
    "- file3.txt - Letter of Appointment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a1bcbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\new_works\\nlp-projects\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stored 9 embeddings in ChromaDB collection 'document_types'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def embed_text(text, model=\"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Embeds text using a sentence transformer model.\n",
    "    \"\"\"\n",
    "    embedder = SentenceTransformer(model)\n",
    "    return embedder.encode(text, convert_to_numpy=True).tolist()\n",
    "\n",
    "def store_embeddings_in_chromadb(classification_json_path='data/classification.json', \n",
    "                                embedder_model=\"all-MiniLM-L6-v2\", \n",
    "                                collection_name=\"document_types\"):\n",
    "    \"\"\"\n",
    "    Embeds document types from classification.json and stores them in ChromaDB.\n",
    "    \"\"\"\n",
    "    # Initialize ChromaDB client\n",
    "    client = chromadb.PersistentClient(path=\"./data/chromadb_data\")\n",
    "    \n",
    "    # Create or get collection\n",
    "    try:\n",
    "        collection = client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}  # Use cosine similarity\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating collection: {e}\")\n",
    "        return\n",
    "\n",
    "    # Load classification data\n",
    "    with open(classification_json_path, 'r') as f:\n",
    "        classification_context = json.load(f)\n",
    "    \n",
    "    # Prepare data for ChromaDB\n",
    "    documents = []\n",
    "    embeddings = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    \n",
    "    for idx, (doc_type, desc) in enumerate(classification_context.items()):\n",
    "        embedding = embed_text(desc, embedder_model)\n",
    "        documents.append(desc)\n",
    "        embeddings.append(embedding)\n",
    "        metadatas.append({\"doc_type\": doc_type})\n",
    "        ids.append(f\"doc_{idx}\")\n",
    "    \n",
    "    # Store in ChromaDB\n",
    "    try:\n",
    "        collection.add(\n",
    "            documents=documents,\n",
    "            embeddings=embeddings,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        print(f\"Successfully stored {len(documents)} embeddings in ChromaDB collection '{collection_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error storing embeddings: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    store_embeddings_in_chromadb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803ba1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Name: document_types, Metadata: {'hnsw:space': 'cosine'}\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=\"./data/chromadb_data\")\n",
    "# get the collection print collection name\n",
    "collections = client.list_collections() \n",
    "for collection in collections:\n",
    "    print(f\"Collection Name: {collection.name}, Metadata: {collection.metadata}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099d3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def embed_text(text, model=\"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Embeds text using a sentence transformer model.\n",
    "    \"\"\"\n",
    "    embedder = SentenceTransformer(model)\n",
    "    return embedder.encode(text, convert_to_numpy=True).tolist()\n",
    "\n",
    "def retrieve_relevant_context(document_text, collection_name=\"document_types\", \n",
    "                            embedder_model=\"all-MiniLM-L6-v2\", top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieves the top_k most relevant document type descriptions from ChromaDB.\n",
    "    \"\"\"\n",
    "    # Initialize ChromaDB client\n",
    "    client = chromadb.PersistentClient(path=\"./data/chromadb_data\")\n",
    "    \n",
    "    # Get collection\n",
    "    try:\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing collection: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Embed input document\n",
    "    doc_embedding = embed_text(document_text, embedder_model)\n",
    "    \n",
    "    # Query ChromaDB\n",
    "    try:\n",
    "        results = collection.query(\n",
    "            query_embeddings=[doc_embedding],\n",
    "            n_results=top_k,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying ChromaDB: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Format results\n",
    "    relevant_context = []\n",
    "    for doc, metadata, distance in zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0]):\n",
    "        similarity = 1 - distance  # Convert distance to similarity (for cosine)\n",
    "        relevant_context.append((metadata[\"doc_type\"], doc, similarity))\n",
    "    \n",
    "    return relevant_context\n",
    "\n",
    "def getModelResponse(document_text, model=\"phi4-mini\", collection_name=\"document_types\", top_k=3):\n",
    "    \"\"\"\n",
    "    Gets a classification response from the Ollama model using RAG with ChromaDB.\n",
    "    Returns the predicted document type.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant context from ChromaDB\n",
    "    relevant_context = retrieve_relevant_context(document_text, collection_name, top_k=top_k)\n",
    "    \n",
    "    # Build context string from retrieved documents\n",
    "    context_str = \"Relevant Document Types and Descriptions:\\n\"\n",
    "    for doc_type, desc, score in relevant_context:\n",
    "        context_str += f\"- {doc_type}: {desc} (Similarity: {score:.2f})\\n\"\n",
    "    \n",
    "    # Construct prompt with retrieved context\n",
    "    prompt = (\n",
    "        f\"{context_str}\\n\"\n",
    "        \"Based on the above relevant document types, classify the following document and return only the type:\\n\\n\"\n",
    "        f\"{document_text}\"\n",
    "    )\n",
    "\n",
    "    # Get response from Ollama model\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': prompt},\n",
    "        ]\n",
    "    )\n",
    "    doc_type = response['message']['content'].strip()\n",
    "\n",
    "    return doc_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a501f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi4-mini Document Type: Job Post. (Similarity: 0.15)  Execution Duration: 11.20 seconds\n",
      "llama3 Document Type: Resume  Execution Duration: 30.42 seconds\n",
      "gemma3 Document Type: Resume  Execution Duration: 12.51 seconds\n",
      "mistral:7b Document Type: Resume  Execution Duration: 34.09 seconds\n",
      "\t\t****** end o file process data/file1.txt *******\n",
      "\n",
      "phi4-mini Document Type: Job Post  Execution Duration: 13.51 seconds\n",
      "llama3 Document Type: Job Post  Execution Duration: 26.58 seconds\n",
      "gemma3 Document Type: Job Post  Execution Duration: 11.93 seconds\n",
      "mistral:7b Document Type: Job Post  Execution Duration: 28.30 seconds\n",
      "\t\t****** end o file process data/file2.txt *******\n",
      "\n",
      "phi4-mini Document Type: Job Post  Execution Duration: 12.57 seconds\n",
      "llama3 Document Type: Job Post  Execution Duration: 23.83 seconds\n",
      "gemma3 Document Type: Job Post  Execution Duration: 9.95 seconds\n",
      "mistral:7b Document Type: Job Post  Execution Duration: 24.06 seconds\n",
      "\t\t****** end o file process data/file3.txt *******\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "files = ['file1.txt', 'file2.txt', 'file3.txt']\n",
    "models = ['phi4-mini', 'llama3', 'gemma3','mistral:7b']\n",
    "# models = ['llama3']\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    file_path = f'data/{file}'\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read().strip()\n",
    "\n",
    "        for model in models:\n",
    "            start_time = time.time()\n",
    "            doctype = getModelResponse( content, collection_name=\"document_types\", top_k=3, model=model)\n",
    "            end_time = time.time()\n",
    "            print(f\"{model} Document Type: {doctype}  Execution Duration: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "        print(f\"\\t\\t****** end o file process {file_path} *******\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd501b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi4-mini Document Type: Resume  Execution Duration: 20.36 seconds\n",
      "llama3 Document Type: Resume  Execution Duration: 41.14 seconds\n",
      "gemma3 Document Type: Resume  Execution Duration: 17.11 seconds\n",
      "mistral:7b Document Type: The document provided is a Resume.  Execution Duration: 44.91 seconds\n",
      "\t\t****** end o file process data/file1.txt *******\n",
      "\n",
      "phi4-mini Document Type: Job Post  Execution Duration: 17.39 seconds\n",
      "llama3 Document Type: Job Post  Execution Duration: 37.30 seconds\n",
      "gemma3 Document Type: Job Post  Execution Duration: 15.57 seconds\n",
      "mistral:7b Document Type: The document type is \"Job Post\" (Similarity: 0.31)  Execution Duration: 40.11 seconds\n",
      "\t\t****** end o file process data/file2.txt *******\n",
      "\n",
      "phi4-mini Document Type: Letter  Execution Duration: 16.77 seconds\n",
      "llama3 Document Type: Letter  Execution Duration: 33.17 seconds\n",
      "gemma3 Document Type: Job Post  Execution Duration: 14.21 seconds\n",
      "mistral:7b Document Type: Job Post  Execution Duration: 35.39 seconds\n",
      "\t\t****** end o file process data/file3.txt *******\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "files = ['file1.txt', 'file2.txt', 'file3.txt']\n",
    "models = ['phi4-mini', 'llama3', 'gemma3','mistral:7b']\n",
    "# models = ['llama3']\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    file_path = f'data/{file}'\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read().strip()\n",
    "\n",
    "        for model in models:\n",
    "            start_time = time.time()\n",
    "            doctype = getModelResponse( content, collection_name=\"document_types\", top_k=10  , model=model)\n",
    "            end_time = time.time()\n",
    "            print(f\"{model} Document Type: {doctype}  Execution Duration: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "        print(f\"\\t\\t****** end o file process {file_path} *******\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
